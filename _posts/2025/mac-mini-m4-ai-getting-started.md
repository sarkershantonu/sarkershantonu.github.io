### M4 Mac Mini Specification

### Software 
### AI Platforms 
### LLMs 

### The MLX Platform : 
Apple provides opensource Platform , Named MLX. This is solo designed for Apple Silicon M1 to M5 chip. This SOC has CPU, GPU, Unified Memory all built-in. 

### MLX : Language Support   
- Swift 
- C 
- C++

### [MLX : Documentation](https://ml-explore.github.io/mlx/build/html/index.html)
- Available via python package manager PyPi
- Current Requirements MacOs 14.0+ & Python 3.10
### MLS : Installation 

##### MLS : Installation | Dependencies
- Check MAC version, in my case  
![MacVersion]()
- Check Python version, if not present , install python 
- if you are using non-native python, switch to a native python 


### How to check your python is native python in MacOS? 
- See version by ```python3 --version``` , expected 
- check installation places ```which python3```
### Build MLX from source 

Local MLX https://www.youtube.com/watch?v=mStqWk0aCc4  

Setting up Model Running Software : 
### MacOs : Ollama Basics 
- Installation
  - [Download](https://ollama.com/download/Ollama.dmg) and install . Or 
- Run LLM Locally 
- Expose LLM in Local Network (LAN)
- How to keep LLMs disconnected from Internet 
- Local LLM collections 
### MacOs : LMStudio Basics

### MacOs : vLLMs Basics

Claude Code Basics 
- in terminal ```curl -fsSL https://claude.ai/install.sh | bash```